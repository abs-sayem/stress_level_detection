{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load the TESS Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Organize the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the TESS dataset\n",
    "dataset_path = \"dataset/tess_dataset\"\n",
    "\n",
    "# List of all emotions in the dataset (folder names)\n",
    "emotions = os.listdir(dataset_path)\n",
    "\n",
    "# Initialize lists to store features and labels\n",
    "audio_features = []\n",
    "emotion_labels = []\n",
    "\n",
    "# Function to extract features (MFCCs) from an audio file\n",
    "def extract_features(audio_path, sr=22050, n_mfcc=13):\n",
    "    y, sr = librosa.load(audio_path, sr=sr)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfcc.T, axis=0)  # Take the mean along time axis\n",
    "\n",
    "# Loop through each emotion folder\n",
    "for emotion in emotions:\n",
    "    emotion_folder = os.path.join(dataset_path, emotion)\n",
    "    \n",
    "    # Loop through each audio file in the folder\n",
    "    for file in os.listdir(emotion_folder):\n",
    "        if file.endswith(\".wav\"):  # Process only .wav files\n",
    "            file_path = os.path.join(emotion_folder, file)\n",
    "            \n",
    "            # Extract features and append them\n",
    "            features = extract_features(file_path)\n",
    "            audio_features.append(features)\n",
    "            \n",
    "            # Append the corresponding emotion label\n",
    "            emotion_labels.append(emotion)\n",
    "\n",
    "# Convert to numpy arrays for training\n",
    "audio_features = np.array(audio_features)\n",
    "emotion_labels = np.array(emotion_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Organize and Save the CSV Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as 'processed_tess_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save to a CSV file for easy access\n",
    "dataset = pd.DataFrame(audio_features)\n",
    "dataset['Label'] = emotion_labels\n",
    "dataset.to_csv(\"dataset/processed_tess_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Dataset saved as 'processed_tess_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Organizing the Processed Data for Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (2240, 13)\n",
      "Training Labels Shape: (2240, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Encode labels (convert text labels to numbers)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "emotion_labels_encoded = label_encoder.fit_transform(emotion_labels)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    audio_features, emotion_labels_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# One-hot encode the labels for training\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(f\"Training Features Shape: {X_train.shape}\")\n",
    "print(f\"Training Labels Shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess the Audio Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading dataset/tess_dataset\\TESS Toronto emotional speech set data\\OAF_angry: [Errno 13] Permission denied: 'dataset/tess_dataset\\\\TESS Toronto emotional speech set data\\\\OAF_angry'\n",
      "Error loading dataset/tess_dataset\\TESS Toronto emotional speech set data\\OAF_disgust: [Errno 13] Permission denied: 'dataset/tess_dataset\\\\TESS Toronto emotional speech set data\\\\OAF_disgust'\n",
      "Error loading dataset/tess_dataset\\TESS Toronto emotional speech set data\\OAF_Fear: [Errno 13] Permission denied: 'dataset/tess_dataset\\\\TESS Toronto emotional speech set data\\\\OAF_Fear'\n",
      "Error loading dataset/tess_dataset\\TESS Toronto emotional speech set data\\OAF_happy: [Errno 13] Permission denied: 'dataset/tess_dataset\\\\TESS Toronto emotional speech set data\\\\OAF_happy'\n",
      "Error loading dataset/tess_dataset\\TESS Toronto emotional speech set data\\OAF_neutral: [Errno 13] Permission denied: 'dataset/tess_dataset\\\\TESS Toronto emotional speech set data\\\\OAF_neutral'\n",
      "Error loading dataset/tess_dataset\\TESS Toronto emotional speech set data\\OAF_Pleasant_surprise: [Errno 13] Permission denied: 'dataset/tess_dataset\\\\TESS Toronto emotional speech set data\\\\OAF_Pleasant_surprise'\n",
      "Error loading dataset/tess_dataset\\TESS Toronto emotional speech set data\\OAF_Sad: [Errno 13] Permission denied: 'dataset/tess_dataset\\\\TESS Toronto emotional speech set data\\\\OAF_Sad'\n",
      "Error loading dataset/tess_dataset\\TESS Toronto emotional speech set data\\YAF_angry: [Errno 13] Permission denied: 'dataset/tess_dataset\\\\TESS Toronto emotional speech set data\\\\YAF_angry'\n",
      "Error loading dataset/tess_dataset\\TESS Toronto emotional speech set data\\YAF_disgust: [Errno 13] Permission denied: 'dataset/tess_dataset\\\\TESS Toronto emotional speech set data\\\\YAF_disgust'\n",
      "Error loading dataset/tess_dataset\\TESS Toronto emotional speech set data\\YAF_fear: [Errno 13] Permission denied: 'dataset/tess_dataset\\\\TESS Toronto emotional speech set data\\\\YAF_fear'\n",
      "Error loading dataset/tess_dataset\\TESS Toronto emotional speech set data\\YAF_happy: [Errno 13] Permission denied: 'dataset/tess_dataset\\\\TESS Toronto emotional speech set data\\\\YAF_happy'\n",
      "Error loading dataset/tess_dataset\\TESS Toronto emotional speech set data\\YAF_neutral: [Errno 13] Permission denied: 'dataset/tess_dataset\\\\TESS Toronto emotional speech set data\\\\YAF_neutral'\n",
      "Error loading dataset/tess_dataset\\TESS Toronto emotional speech set data\\YAF_pleasant_surprised: [Errno 13] Permission denied: 'dataset/tess_dataset\\\\TESS Toronto emotional speech set data\\\\YAF_pleasant_surprised'\n",
      "Error loading dataset/tess_dataset\\TESS Toronto emotional speech set data\\YAF_sad: [Errno 13] Permission denied: 'dataset/tess_dataset\\\\TESS Toronto emotional speech set data\\\\YAF_sad'\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def extract_features(audio_path, sr=22050, n_mfcc=13):\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=sr)  # Default method\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {audio_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract MFCC features\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfcc.T, axis=0)\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"dataset/tess_dataset\"\n",
    "features, labels = [], []\n",
    "\n",
    "for label in os.listdir(dataset_path):\n",
    "    for file in os.listdir(os.path.join(dataset_path, label)):\n",
    "        file_path = os.path.join(dataset_path, label, file)\n",
    "        feature = extract_features(file_path)\n",
    "        if feature is not None:  # Skip files that failed to load\n",
    "            features.append(feature)\n",
    "            labels.append(label)\n",
    "\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Shape: (2800, 13)\n"
     ]
    }
   ],
   "source": [
    "#features = np.expand_dims(features, axis=-1)  # Add channel dimension\n",
    "print(f\"Feature Shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Build the CNN+LSTM Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN + LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Flatten\n",
    "\n",
    "def create_cnn_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')  # Output emotion probabilities\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the Model**\n",
    "\n",
    "Here we use-\n",
    "- k-fold cross validation (5 folds) for 100 epoch of each.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Epoch 1/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.3827 - loss: 2.0229 - val_accuracy: 0.8268 - val_loss: 0.5911\n",
      "Epoch 2/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8388 - loss: 0.5209 - val_accuracy: 0.8804 - val_loss: 0.3722\n",
      "Epoch 3/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8611 - loss: 0.3868 - val_accuracy: 0.8893 - val_loss: 0.2986\n",
      "Epoch 4/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8972 - loss: 0.2950 - val_accuracy: 0.9018 - val_loss: 0.2928\n",
      "Epoch 5/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.8940 - loss: 0.2882 - val_accuracy: 0.8929 - val_loss: 0.2923\n",
      "Epoch 6/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9173 - loss: 0.2320 - val_accuracy: 0.9214 - val_loss: 0.2361\n",
      "Epoch 7/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9266 - loss: 0.2082 - val_accuracy: 0.9089 - val_loss: 0.2885\n",
      "Epoch 8/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9197 - loss: 0.2141 - val_accuracy: 0.9036 - val_loss: 0.2724\n",
      "Epoch 9/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9224 - loss: 0.2039 - val_accuracy: 0.9393 - val_loss: 0.1995\n",
      "Epoch 10/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9338 - loss: 0.1748 - val_accuracy: 0.9232 - val_loss: 0.2109\n",
      "Epoch 11/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9476 - loss: 0.1446 - val_accuracy: 0.9071 - val_loss: 0.2473\n",
      "Epoch 12/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9549 - loss: 0.1285 - val_accuracy: 0.8821 - val_loss: 0.3437\n",
      "Epoch 13/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9463 - loss: 0.1437 - val_accuracy: 0.9250 - val_loss: 0.2076\n",
      "Epoch 14/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9474 - loss: 0.1461 - val_accuracy: 0.9161 - val_loss: 0.2281\n",
      "Epoch 15/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9591 - loss: 0.1295 - val_accuracy: 0.9196 - val_loss: 0.2169\n",
      "Epoch 16/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9579 - loss: 0.1206 - val_accuracy: 0.9125 - val_loss: 0.2371\n",
      "Epoch 17/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9420 - loss: 0.1443 - val_accuracy: 0.9268 - val_loss: 0.2117\n",
      "Epoch 18/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9544 - loss: 0.1264 - val_accuracy: 0.9232 - val_loss: 0.2275\n",
      "Epoch 19/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9656 - loss: 0.1043 - val_accuracy: 0.9196 - val_loss: 0.2126\n",
      "Fold 1: Loss = 0.1995, Accuracy = 0.9393\n",
      "\n",
      "Fold 2/5\n",
      "Epoch 1/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.3680 - loss: 2.0626 - val_accuracy: 0.8018 - val_loss: 0.6741\n",
      "Epoch 2/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8134 - loss: 0.5642 - val_accuracy: 0.8250 - val_loss: 0.4564\n",
      "Epoch 3/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8753 - loss: 0.3691 - val_accuracy: 0.9089 - val_loss: 0.3341\n",
      "Epoch 4/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8975 - loss: 0.3057 - val_accuracy: 0.8946 - val_loss: 0.3017\n",
      "Epoch 5/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9036 - loss: 0.2629 - val_accuracy: 0.9000 - val_loss: 0.3035\n",
      "Epoch 6/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9210 - loss: 0.2155 - val_accuracy: 0.9161 - val_loss: 0.2425\n",
      "Epoch 7/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9331 - loss: 0.2086 - val_accuracy: 0.9107 - val_loss: 0.2673\n",
      "Epoch 8/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9343 - loss: 0.1895 - val_accuracy: 0.9018 - val_loss: 0.2914\n",
      "Epoch 9/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9325 - loss: 0.1830 - val_accuracy: 0.9089 - val_loss: 0.2589\n",
      "Epoch 10/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9343 - loss: 0.1965 - val_accuracy: 0.9018 - val_loss: 0.2750\n",
      "Epoch 11/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9381 - loss: 0.1695 - val_accuracy: 0.9125 - val_loss: 0.2355\n",
      "Epoch 12/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9476 - loss: 0.1416 - val_accuracy: 0.9054 - val_loss: 0.2755\n",
      "Epoch 13/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9479 - loss: 0.1454 - val_accuracy: 0.9214 - val_loss: 0.2555\n",
      "Epoch 14/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9486 - loss: 0.1296 - val_accuracy: 0.9125 - val_loss: 0.2526\n",
      "Epoch 15/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9371 - loss: 0.1652 - val_accuracy: 0.9214 - val_loss: 0.2394\n",
      "Epoch 16/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9458 - loss: 0.1449 - val_accuracy: 0.9000 - val_loss: 0.2929\n",
      "Epoch 17/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9416 - loss: 0.1424 - val_accuracy: 0.8911 - val_loss: 0.3312\n",
      "Epoch 18/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9479 - loss: 0.1337 - val_accuracy: 0.9161 - val_loss: 0.2565\n",
      "Epoch 19/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9569 - loss: 0.1174 - val_accuracy: 0.9161 - val_loss: 0.2384\n",
      "Epoch 20/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9628 - loss: 0.1020 - val_accuracy: 0.8929 - val_loss: 0.2901\n",
      "Epoch 21/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9471 - loss: 0.1331 - val_accuracy: 0.9214 - val_loss: 0.2226\n",
      "Epoch 22/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9491 - loss: 0.1355 - val_accuracy: 0.9214 - val_loss: 0.2276\n",
      "Epoch 23/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9572 - loss: 0.1094 - val_accuracy: 0.9214 - val_loss: 0.2160\n",
      "Epoch 24/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9703 - loss: 0.0989 - val_accuracy: 0.9214 - val_loss: 0.2396\n",
      "Epoch 25/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9713 - loss: 0.0965 - val_accuracy: 0.9107 - val_loss: 0.2915\n",
      "Epoch 26/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9646 - loss: 0.0958 - val_accuracy: 0.9125 - val_loss: 0.2549\n",
      "Epoch 27/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9665 - loss: 0.0885 - val_accuracy: 0.9268 - val_loss: 0.2296\n",
      "Epoch 28/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9690 - loss: 0.0916 - val_accuracy: 0.9125 - val_loss: 0.2961\n",
      "Epoch 29/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9707 - loss: 0.0828 - val_accuracy: 0.8982 - val_loss: 0.3216\n",
      "Epoch 30/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9692 - loss: 0.0832 - val_accuracy: 0.9321 - val_loss: 0.2189\n",
      "Epoch 31/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9705 - loss: 0.0777 - val_accuracy: 0.9214 - val_loss: 0.2449\n",
      "Epoch 32/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9805 - loss: 0.0522 - val_accuracy: 0.9089 - val_loss: 0.2643\n",
      "Epoch 33/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9780 - loss: 0.0590 - val_accuracy: 0.9107 - val_loss: 0.2543\n",
      "Fold 2: Loss = 0.2160, Accuracy = 0.9214\n",
      "\n",
      "Fold 3/5\n",
      "Epoch 1/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.3788 - loss: 2.0729 - val_accuracy: 0.7571 - val_loss: 0.7489\n",
      "Epoch 2/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8184 - loss: 0.5718 - val_accuracy: 0.8518 - val_loss: 0.4102\n",
      "Epoch 3/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8680 - loss: 0.3766 - val_accuracy: 0.8982 - val_loss: 0.3061\n",
      "Epoch 4/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9036 - loss: 0.2832 - val_accuracy: 0.8696 - val_loss: 0.3160\n",
      "Epoch 5/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9034 - loss: 0.2752 - val_accuracy: 0.8964 - val_loss: 0.2960\n",
      "Epoch 6/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9138 - loss: 0.2535 - val_accuracy: 0.9000 - val_loss: 0.2793\n",
      "Epoch 7/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9136 - loss: 0.2290 - val_accuracy: 0.9161 - val_loss: 0.2177\n",
      "Epoch 8/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9254 - loss: 0.1929 - val_accuracy: 0.8893 - val_loss: 0.3138\n",
      "Epoch 9/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9132 - loss: 0.2172 - val_accuracy: 0.9161 - val_loss: 0.2365\n",
      "Epoch 10/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9141 - loss: 0.2053 - val_accuracy: 0.9125 - val_loss: 0.2408\n",
      "Epoch 11/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9366 - loss: 0.1681 - val_accuracy: 0.9232 - val_loss: 0.2050\n",
      "Epoch 12/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9522 - loss: 0.1345 - val_accuracy: 0.9036 - val_loss: 0.2293\n",
      "Epoch 13/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9480 - loss: 0.1426 - val_accuracy: 0.9143 - val_loss: 0.2072\n",
      "Epoch 14/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9528 - loss: 0.1491 - val_accuracy: 0.9107 - val_loss: 0.2238\n",
      "Epoch 15/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9344 - loss: 0.1614 - val_accuracy: 0.9250 - val_loss: 0.2121\n",
      "Epoch 16/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9450 - loss: 0.1485 - val_accuracy: 0.9054 - val_loss: 0.2278\n",
      "Epoch 17/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9440 - loss: 0.1372 - val_accuracy: 0.9250 - val_loss: 0.2260\n",
      "Epoch 18/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9592 - loss: 0.1181 - val_accuracy: 0.9196 - val_loss: 0.2012\n",
      "Epoch 19/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9547 - loss: 0.1283 - val_accuracy: 0.9196 - val_loss: 0.2246\n",
      "Epoch 20/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.9604 - loss: 0.1112 - val_accuracy: 0.9161 - val_loss: 0.2051\n",
      "Epoch 21/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9535 - loss: 0.1130 - val_accuracy: 0.9161 - val_loss: 0.2232\n",
      "Epoch 22/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9637 - loss: 0.1055 - val_accuracy: 0.9214 - val_loss: 0.2108\n",
      "Epoch 23/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9599 - loss: 0.1065 - val_accuracy: 0.9268 - val_loss: 0.1846\n",
      "Epoch 24/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9734 - loss: 0.0764 - val_accuracy: 0.9179 - val_loss: 0.2149\n",
      "Epoch 25/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9618 - loss: 0.0985 - val_accuracy: 0.9214 - val_loss: 0.2076\n",
      "Epoch 26/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9694 - loss: 0.0975 - val_accuracy: 0.9321 - val_loss: 0.1820\n",
      "Epoch 27/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9679 - loss: 0.0823 - val_accuracy: 0.9143 - val_loss: 0.2193\n",
      "Epoch 28/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9738 - loss: 0.0797 - val_accuracy: 0.9107 - val_loss: 0.2616\n",
      "Epoch 29/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9570 - loss: 0.1128 - val_accuracy: 0.9304 - val_loss: 0.1995\n",
      "Epoch 30/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9744 - loss: 0.0713 - val_accuracy: 0.9286 - val_loss: 0.2155\n",
      "Epoch 31/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9734 - loss: 0.0765 - val_accuracy: 0.9196 - val_loss: 0.2269\n",
      "Epoch 32/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9701 - loss: 0.0738 - val_accuracy: 0.9250 - val_loss: 0.2211\n",
      "Epoch 33/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9698 - loss: 0.0802 - val_accuracy: 0.9107 - val_loss: 0.2295\n",
      "Epoch 34/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9812 - loss: 0.0540 - val_accuracy: 0.9214 - val_loss: 0.2131\n",
      "Epoch 35/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9742 - loss: 0.0735 - val_accuracy: 0.9268 - val_loss: 0.1777\n",
      "Epoch 36/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9776 - loss: 0.0572 - val_accuracy: 0.9339 - val_loss: 0.1726\n",
      "Epoch 37/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9773 - loss: 0.0589 - val_accuracy: 0.9250 - val_loss: 0.2366\n",
      "Epoch 38/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9824 - loss: 0.0537 - val_accuracy: 0.9232 - val_loss: 0.2166\n",
      "Epoch 39/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9830 - loss: 0.0453 - val_accuracy: 0.9232 - val_loss: 0.2119\n",
      "Epoch 40/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9718 - loss: 0.0747 - val_accuracy: 0.9196 - val_loss: 0.2665\n",
      "Epoch 41/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9836 - loss: 0.0471 - val_accuracy: 0.9232 - val_loss: 0.2314\n",
      "Epoch 42/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9891 - loss: 0.0374 - val_accuracy: 0.9214 - val_loss: 0.2085\n",
      "Epoch 43/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9836 - loss: 0.0428 - val_accuracy: 0.9000 - val_loss: 0.3479\n",
      "Epoch 44/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9769 - loss: 0.0600 - val_accuracy: 0.9268 - val_loss: 0.2009\n",
      "Epoch 45/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9864 - loss: 0.0410 - val_accuracy: 0.9411 - val_loss: 0.1932\n",
      "Epoch 46/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9708 - loss: 0.0629 - val_accuracy: 0.9268 - val_loss: 0.2549\n",
      "Fold 3: Loss = 0.1726, Accuracy = 0.9339\n",
      "\n",
      "Fold 4/5\n",
      "Epoch 1/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.3484 - loss: 2.0915 - val_accuracy: 0.7696 - val_loss: 0.7378\n",
      "Epoch 2/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8285 - loss: 0.5639 - val_accuracy: 0.8429 - val_loss: 0.4740\n",
      "Epoch 3/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8723 - loss: 0.3795 - val_accuracy: 0.8750 - val_loss: 0.3628\n",
      "Epoch 4/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8852 - loss: 0.3240 - val_accuracy: 0.9036 - val_loss: 0.2866\n",
      "Epoch 5/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9160 - loss: 0.2336 - val_accuracy: 0.8768 - val_loss: 0.3397\n",
      "Epoch 6/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9197 - loss: 0.2210 - val_accuracy: 0.8946 - val_loss: 0.3003\n",
      "Epoch 7/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9149 - loss: 0.2385 - val_accuracy: 0.9107 - val_loss: 0.2562\n",
      "Epoch 8/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9145 - loss: 0.2155 - val_accuracy: 0.9071 - val_loss: 0.2422\n",
      "Epoch 9/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9235 - loss: 0.1862 - val_accuracy: 0.9250 - val_loss: 0.2137\n",
      "Epoch 10/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9406 - loss: 0.1788 - val_accuracy: 0.9250 - val_loss: 0.2056\n",
      "Epoch 11/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9346 - loss: 0.1704 - val_accuracy: 0.9268 - val_loss: 0.2150\n",
      "Epoch 12/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9354 - loss: 0.1707 - val_accuracy: 0.9321 - val_loss: 0.2114\n",
      "Epoch 13/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9499 - loss: 0.1407 - val_accuracy: 0.9214 - val_loss: 0.2246\n",
      "Epoch 14/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9445 - loss: 0.1426 - val_accuracy: 0.9196 - val_loss: 0.2244\n",
      "Epoch 15/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9415 - loss: 0.1431 - val_accuracy: 0.9339 - val_loss: 0.2272\n",
      "Epoch 16/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9454 - loss: 0.1360 - val_accuracy: 0.9196 - val_loss: 0.2160\n",
      "Epoch 17/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9445 - loss: 0.1459 - val_accuracy: 0.9125 - val_loss: 0.2650\n",
      "Epoch 18/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9558 - loss: 0.1221 - val_accuracy: 0.9143 - val_loss: 0.2415\n",
      "Epoch 19/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9496 - loss: 0.1296 - val_accuracy: 0.9196 - val_loss: 0.2533\n",
      "Epoch 20/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9610 - loss: 0.1085 - val_accuracy: 0.9357 - val_loss: 0.2093\n",
      "Fold 4: Loss = 0.2056, Accuracy = 0.9250\n",
      "\n",
      "Fold 5/5\n",
      "Epoch 1/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.3497 - loss: 2.1351 - val_accuracy: 0.7857 - val_loss: 0.6940\n",
      "Epoch 2/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7933 - loss: 0.6337 - val_accuracy: 0.8429 - val_loss: 0.4401\n",
      "Epoch 3/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8607 - loss: 0.4081 - val_accuracy: 0.8321 - val_loss: 0.4321\n",
      "Epoch 4/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8823 - loss: 0.3287 - val_accuracy: 0.8839 - val_loss: 0.3017\n",
      "Epoch 5/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9147 - loss: 0.2488 - val_accuracy: 0.8696 - val_loss: 0.3349\n",
      "Epoch 6/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9057 - loss: 0.2574 - val_accuracy: 0.8911 - val_loss: 0.2788\n",
      "Epoch 7/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9220 - loss: 0.2160 - val_accuracy: 0.8679 - val_loss: 0.3358\n",
      "Epoch 8/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9125 - loss: 0.2324 - val_accuracy: 0.9179 - val_loss: 0.2352\n",
      "Epoch 9/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9358 - loss: 0.1919 - val_accuracy: 0.8821 - val_loss: 0.2949\n",
      "Epoch 10/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9458 - loss: 0.1600 - val_accuracy: 0.8804 - val_loss: 0.2951\n",
      "Epoch 11/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9261 - loss: 0.1835 - val_accuracy: 0.9018 - val_loss: 0.2599\n",
      "Epoch 12/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9281 - loss: 0.1859 - val_accuracy: 0.8750 - val_loss: 0.3450\n",
      "Epoch 13/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9193 - loss: 0.2126 - val_accuracy: 0.9036 - val_loss: 0.2359\n",
      "Epoch 14/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9490 - loss: 0.1561 - val_accuracy: 0.8625 - val_loss: 0.3800\n",
      "Epoch 15/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9380 - loss: 0.1671 - val_accuracy: 0.8982 - val_loss: 0.2663\n",
      "Epoch 16/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9524 - loss: 0.1216 - val_accuracy: 0.9125 - val_loss: 0.2426\n",
      "Epoch 17/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9415 - loss: 0.1454 - val_accuracy: 0.9250 - val_loss: 0.2159\n",
      "Epoch 18/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9470 - loss: 0.1299 - val_accuracy: 0.9054 - val_loss: 0.2630\n",
      "Epoch 19/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9638 - loss: 0.1068 - val_accuracy: 0.8714 - val_loss: 0.3377\n",
      "Epoch 20/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9504 - loss: 0.1424 - val_accuracy: 0.8982 - val_loss: 0.2560\n",
      "Epoch 21/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9615 - loss: 0.1089 - val_accuracy: 0.8946 - val_loss: 0.3006\n",
      "Epoch 22/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9626 - loss: 0.1125 - val_accuracy: 0.8875 - val_loss: 0.2932\n",
      "Epoch 23/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9522 - loss: 0.1119 - val_accuracy: 0.8929 - val_loss: 0.2737\n",
      "Epoch 24/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9639 - loss: 0.1050 - val_accuracy: 0.8964 - val_loss: 0.2778\n",
      "Epoch 25/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9593 - loss: 0.1030 - val_accuracy: 0.9036 - val_loss: 0.2850\n",
      "Epoch 26/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9575 - loss: 0.1150 - val_accuracy: 0.9071 - val_loss: 0.2689\n",
      "Epoch 27/200\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9656 - loss: 0.0846 - val_accuracy: 0.8875 - val_loss: 0.2962\n",
      "Fold 5: Loss = 0.2159, Accuracy = 0.9250\n",
      "\n",
      "Cross-Validation Results:\n",
      "Average Accuracy: 0.9289 ± 0.0066\n",
      "Average Loss: 0.2019 ± 0.0160\n",
      "\n",
      "Training Final Model on Entire Dataset...\n",
      "Epoch 1/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.4046 - loss: 1.9020\n",
      "Epoch 2/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8551 - loss: 0.4619\n",
      "Epoch 3/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8815 - loss: 0.3367\n",
      "Epoch 4/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9081 - loss: 0.2693\n",
      "Epoch 5/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9080 - loss: 0.2501\n",
      "Epoch 6/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9224 - loss: 0.2186\n",
      "Epoch 7/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9106 - loss: 0.2279\n",
      "Epoch 8/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9299 - loss: 0.1968\n",
      "Epoch 9/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9287 - loss: 0.2073\n",
      "Epoch 10/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9257 - loss: 0.1903\n",
      "Epoch 11/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9332 - loss: 0.1884\n",
      "Epoch 12/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9393 - loss: 0.1627\n",
      "Epoch 13/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9339 - loss: 0.1684\n",
      "Epoch 14/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9317 - loss: 0.1768\n",
      "Epoch 15/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9399 - loss: 0.1489\n",
      "Epoch 16/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9574 - loss: 0.1264\n",
      "Epoch 17/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9656 - loss: 0.1034\n",
      "Epoch 18/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9512 - loss: 0.1305\n",
      "Epoch 19/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9592 - loss: 0.1118\n",
      "Epoch 20/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9568 - loss: 0.1190\n",
      "Epoch 21/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9546 - loss: 0.1148\n",
      "Epoch 22/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9575 - loss: 0.1085\n",
      "Epoch 23/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9671 - loss: 0.0981\n",
      "Epoch 24/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9704 - loss: 0.0882\n",
      "Epoch 25/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9647 - loss: 0.0928\n",
      "Epoch 26/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9593 - loss: 0.1075\n",
      "Epoch 27/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9616 - loss: 0.0995\n",
      "Epoch 28/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9704 - loss: 0.0799\n",
      "Epoch 29/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9571 - loss: 0.1038\n",
      "Epoch 30/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9721 - loss: 0.0716\n",
      "Epoch 31/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9718 - loss: 0.0769\n",
      "Epoch 32/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9846 - loss: 0.0486\n",
      "Epoch 33/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9748 - loss: 0.0678\n",
      "Epoch 34/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9780 - loss: 0.0561\n",
      "Epoch 35/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9729 - loss: 0.0649\n",
      "Epoch 36/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9793 - loss: 0.0618\n",
      "Epoch 37/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9809 - loss: 0.0525\n",
      "Epoch 38/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9872 - loss: 0.0580\n",
      "Epoch 39/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9881 - loss: 0.0392\n",
      "Epoch 40/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9830 - loss: 0.0488\n",
      "Epoch 41/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9654 - loss: 0.0955\n",
      "Epoch 42/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9831 - loss: 0.0475\n",
      "Epoch 43/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9873 - loss: 0.0422\n",
      "Epoch 44/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9936 - loss: 0.0293\n",
      "Epoch 45/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9829 - loss: 0.0448\n",
      "Epoch 46/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9705 - loss: 0.0746\n",
      "Epoch 47/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9770 - loss: 0.0651\n",
      "Epoch 48/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9810 - loss: 0.0490\n",
      "Epoch 49/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9898 - loss: 0.0264\n",
      "Epoch 50/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9880 - loss: 0.0387\n",
      "Epoch 51/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9852 - loss: 0.0354\n",
      "Epoch 52/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9890 - loss: 0.0355\n",
      "Epoch 53/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9943 - loss: 0.0186\n",
      "Epoch 54/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9980 - loss: 0.0110\n",
      "Epoch 55/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9994 - loss: 0.0071\n",
      "Epoch 56/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9832 - loss: 0.0559\n",
      "Epoch 57/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9719 - loss: 0.0731\n",
      "Epoch 58/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9759 - loss: 0.0578\n",
      "Epoch 59/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9867 - loss: 0.0330\n",
      "Epoch 60/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9894 - loss: 0.0290\n",
      "Epoch 61/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9973 - loss: 0.0109\n",
      "Epoch 62/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9924 - loss: 0.0213\n",
      "Epoch 63/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9916 - loss: 0.0264\n",
      "Epoch 64/200\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9969 - loss: 0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved at: models\\final_cnnlstm_model.h5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Number of folds\n",
    "k_folds = 5\n",
    "\n",
    "# Create a directory to save models\n",
    "output_dir = \"models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Convert labels to categorical\n",
    "unique_labels = list(set(labels))\n",
    "label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
    "y_encoded = np.array([label_to_index[label] for label in labels])\n",
    "y_categorical = to_categorical(y_encoded, num_classes=len(unique_labels))\n",
    "\n",
    "# Reshape features for CNN+LSTM\n",
    "features = np.expand_dims(features, axis=-1)  # Add a channel dimension\n",
    "\n",
    "# Initialize K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "fold_accuracy = []\n",
    "fold_loss = []\n",
    "\n",
    "# K-Fold Training\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(features)):\n",
    "    print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "    \n",
    "    # Split data for this fold\n",
    "    X_train, X_val = features[train_idx], features[val_idx]\n",
    "    y_train, y_val = y_categorical[train_idx], y_categorical[val_idx]\n",
    "    \n",
    "    # Create and compile a new model instance for each fold\n",
    "    model = create_cnn_lstm_model(input_shape=(X_train.shape[1], 1), num_classes=len(unique_labels))\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    # Define Early Stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\",  # Monitor validation loss\n",
    "        patience=10,          # Stop after 10 epochs of no improvement\n",
    "        restore_best_weights=True  # Restore the weights of the best epoch\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200,  # Set a higher max epochs; early stopping will stop it early\n",
    "        batch_size=16,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping],  # Add EarlyStopping callback\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "    fold_accuracy.append(accuracy)\n",
    "    fold_loss.append(loss)\n",
    "    print(f\"Fold {fold + 1}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "# Final Cross-Validation Results\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Average Accuracy: {np.mean(fold_accuracy):.4f} ± {np.std(fold_accuracy):.4f}\")\n",
    "print(f\"Average Loss: {np.mean(fold_loss):.4f} ± {np.std(fold_loss):.4f}\")\n",
    "\n",
    "# Train Final Model on All Data\n",
    "print(\"\\nTraining Final Model on Entire Dataset...\")\n",
    "final_model = create_cnn_lstm_model(input_shape=(features.shape[1], 1), num_classes=len(unique_labels))\n",
    "final_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train with Early Stopping\n",
    "early_stopping_final = EarlyStopping(\n",
    "    monitor=\"loss\",  # Monitor training loss for the final model\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history_final = final_model.fit(\n",
    "    features, y_categorical,\n",
    "    epochs=200,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stopping_final],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the Final Model\n",
    "final_model_path = os.path.join(output_dir, \"final_cnnlstm_model.h5\")\n",
    "final_model.save(final_model_path)\n",
    "print(f\"Final model saved at: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Integrate ASR (Whisper)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed Text: say the word\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Function to transcribe the audio to text\n",
    "def transcribe_audio(audio_path):\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio = sr.AudioFile(audio_path)\n",
    "    \n",
    "    with audio as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        \n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "    except sr.UnknownValueError:\n",
    "        text = \"Unable to transcribe\"\n",
    "    except sr.RequestError as e:\n",
    "        text = f\"Error: {e}\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example transcription\n",
    "text = transcribe_audio(\"dataset/test_data/OAF_back_angry.wav\")\n",
    "print(\"Transcribed Text:\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Process with LLM (e.g., GPT-4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Stress Level: 3\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "api_key1 = \"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\"\n",
    "api_key2 = \"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\"\n",
    "api_key3 = \"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\"\n",
    "\n",
    "# Initialize the OpenAI API client with your API key\n",
    "openai.api_key = api_key2\n",
    "\n",
    "def predict_stress_level(emotion_scores, text):\n",
    "    \"\"\"\n",
    "    Predicts the stress level based on emotion scores and text input using GPT.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Given the following emotion scores and text, determine the stress level on a scale from 1 to 9:\n",
    "    Emotion Scores: {emotion_scores}\n",
    "    Text: {text}\n",
    "    Provide the stress level only as an integer.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",  # Specify the model name\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=10\n",
    "    )\n",
    "    return int(response.choices[0].message.content.strip())\n",
    "\n",
    "# Example usage\n",
    "emotion_scores = {\n",
    "    'Angry': 0.3,\n",
    "    'Disgust': 0.1,\n",
    "    'Fear': 0.4,\n",
    "    'Happy': 0.1,\n",
    "    'Neutral': 0.2,\n",
    "    'Pleasant_Surprise': 0.05,\n",
    "    'Sad': 0.25\n",
    "}\n",
    "\n",
    "# Replace this with actual transcribed text from Whisper or other ASR\n",
    "text = text\n",
    "\n",
    "# Predict stress level\n",
    "stress_level = predict_stress_level(emotion_scores, text)\n",
    "print(\"Predicted Stress Level:\", stress_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DE45D81D00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DE45D81D00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emotion Scores        : (Angry - 0.0)\n",
      "Transcribed Text      : government\n",
      "Predicted Stress Level: 1\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "import re\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Suppress TensorFlow logs\n",
    "\n",
    "# Path to the saved model\n",
    "model_path = \"models/final_cnnlstm_model.h5\"\n",
    "# Load the trained model\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Emotion names to standardize\n",
    "emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Pleasant_Surprise\", \"Sad\"]\n",
    "\n",
    "# Function to clean emotion labels\n",
    "def clean_emotion_label(label):\n",
    "    for emotion in emotion_labels:\n",
    "        if emotion.lower() in label.lower():\n",
    "            return emotion\n",
    "    return label\n",
    "\n",
    "# Path to the audio file\n",
    "audio_path = \"dataset/test_data/test1.wav\"\n",
    "\n",
    "# Extract features and get emotion scores\n",
    "audio_features = extract_features(audio_path).reshape(1, -1, 1)\n",
    "emotion_probs = model.predict(audio_features, verbose=0)[0]\n",
    "\n",
    "emotion_scores = {clean_emotion_label(label): round(prob, 2) for label, prob in zip(labels, emotion_probs)}\n",
    "\n",
    "# Transcribe audio\n",
    "transcribed_text = transcribe_audio(audio_path)\n",
    "\n",
    "# Predict stress level\n",
    "stress_level = predict_stress_level(emotion_scores, transcribed_text)\n",
    "\n",
    "# Format emotion scores for output\n",
    "formatted_emotion_scores = ', '.join([f\"({emotion} - {score})\" for emotion, score in emotion_scores.items()])\n",
    "\n",
    "# Print results\n",
    "print(\"\\nEmotion Scores        :\", formatted_emotion_scores)\n",
    "print(\"Transcribed Text      :\", transcribed_text)\n",
    "print(\"Predicted Stress Level:\", stress_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAAB5CAYAAAB4HevDAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAL9FJREFUeJzt3QeYXFXZwPGTkB6T0AKkUSORTmiiBpJIMyhKVZo08UMpKoIIfAgoiBRRQUSkCkbgU6kKCAoEBSK9C6FIQjC0EFMEDITM9/yPc5e7s1N35+7O7vx/zzOQ3Z1yyzvnnvve95zbK5fL5YIkSZIkSZJUZ73r/YaSJEmSJEkSTDxJkiRJkiQpEyaeJEmSJEmSlAkTT5IkSZIkScqEiSdJkiRJkiRlwsSTJEmSJEmSMmHiSZIkSZIkSZkw8SRJkiRJkqRMmHiSJEmSJElSJkw8SZKU99xzz4XDDz88rLvuumHw4MFhwIABYfTo0WHzzTePv7/mmmu6ehEbwrRp00KvXr3io6dYffXV4/rMnDmz4nPvuOOO+NyBAweG+fPnV3z+66+/Hvr16xdfc//998ffTZo0Kf7Mtuwu2DYsM9uqI9uvK5x88slx+fi/JEnqXCaeJEkKIVx77bVhgw02CD/72c9iouATn/hE2G233cKGG24Y/vnPf8bfH3LIIW1e1x0TCOqYyZMnhzXWWCP85z//CVdeeWXF5//qV78K7733Xlh//fXDFlts0SnL2IyJUL6LkiSp8fTp6gWQJKmrvfbaa2H//fcPixcvDkcddVQ49dRTY7VT2kMPPRR+97vfddkyqnGQ5DjooIPCd77znXDppZeGQw89tOzzL7vssvj/L33pSy2/u+KKK8Lbb78dVl111dAT3H777TG5NmrUqNCIqFjcc889w4orrtjViyJJUtMx8SRJanp/+MMfwr///e8wcuTI8MMf/rDoczbddNP4kHDAAQfEYVskJJ944olYLVcMQ+ueeuqpONRu3333bfl9T0k4JdZaa63QyEg4mXSSJKlrONROktT0qHjC8OHDax7ec9ddd7UMv0rmPeLxy1/+ss28OO+//3740Y9+FMaPHx8+9KEPtZkj6dlnn43D+TiJp+Jq2LBhYeuttw5Tp04tugwLFiwIJ5xwQkx6MCdV//79Y/KMYYInnnhirEBJI0nyhS98Ic5bRSJk6NChYc0114xDCm+44YaQtWrXj+3EMrJ9/va3v5V8v6OPPjo+58gjjyxagbPrrruGESNGxHVdaaWVwi677BKmT59el3Vh+XbYYYf4b6qeSkn+9tnPfrZV4qPUEE2q7s4666yY5BwyZEhc9lVWWSXOM3bMMceEefPmVTXnUqW5l/7+97+Hk046KcYKVUp8zgorrBC23Xbb8Jvf/Kbm7VHsc5J5lSo9ChN1rCdDEllvlmvllVcOO+20U/jzn//c5nPZjnz3wHcx/b7p7VJpjqdbb701fOYzn4lxwmfyPeK78uCDDxZ9fnr/PfroozHW2L98B5kj7uyzzw65XK7m7ShJUk9kxZMkqekl1SdPPvlkTFhss802FV/DSTHD8/74xz/GxBVJCH6XGDt2bKvncxLKySnP32qrrcI666wTK2ESv/3tb8N+++0X5w36yEc+EnbccceYWLrvvvvCF7/4xTihdTrBwTCtCRMmxGUmYcYyk3x69dVXwzPPPBPuvffe8M1vfjMsu+yy8fms15QpU2IyaqONNgof+9jHYoKH+atuuumm+O/Pfe5zISu1rN8yyywTn/uDH/wgJvC23HLLNu+3ZMmSloQVw94KE1Kc+Pfu3TtsttlmcXu/9NJLMbn2+9//Plx00UXhwAMP7PA6MXTu5ptvjstx5plnhr59+7b6+zvvvBOuvvrqludWsnTp0vDpT3867iuSgiw3+++NN96IE9+TkNp7773D8ssv3+FlJwF6ySWXxH1B4pLPYRvdeeed8fNJ+PGcjth4443jd6SYGTNmxM9gH6Udf/zxcRnWW2+9mHwjpl944YVYlcjjJz/5Sfj617/e8vxPfepTMYlJ4ogEFT8nqq1wYsgkw2tJJH384x+P7cHTTz8dE3DcUODCCy9sE2MJPpftRDJ1u+22C6+88kq4++67YwzOnj07Lq8kSU0vJ0lSk1u0aFFu1KhRlCfkevXqlZs0aVLulFNOyd100025119/vexrJ06cGF935513Fv37iy++GP/OY/To0bkZM2a0ec7jjz+e69+/f27AgAG5a665ptXfZs6cmdtggw3i6y+//PKW3/NvfjdlypTcu+++2+o177//fm7atGm5xYsXt/xu8uTJ8flTp05t8/nz58/PTZ8+PVct1jVZp2q0Z/2effbZ+Ltll102984777R5zxtuuCH+fdNNN231+wsvvDD+fuzYsbnHHnus1d/uuuuu3JAhQ3L9+vWL75+22mqrxdexv6rFdh8+fHh8XeF6gW3N38aMGRP3SaW4Yfn43fjx43MLFy5s834PPPBAbu7cuW1ii2UvpdR6ER8vvPBCm+c/88wzMU55zX333dfqb+U+r5btN2vWrNyIESPi888777xWf7v55ptzc+bMafOae++9Nzd06NBc3759cy+//HLReGSblnLSSSfF5/D/tFtuuSX+nti87bbbWv3t4osvjn/jM5988smi+4/HBRdc0Opvt99+e2xHlllmmdzs2bMrbg9Jkno6h9pJkpoew96o8vjoRz8aK5MYPkMVBNUnDL1haNwFF1wQq4I64rTTTgtrr712m99///vfj0OsqLqgKipttdVWi5UpOPfcc9sMD6TKorDShiqSiRMnxiFDhc+n0qgQQ96KVRXVS3vW78Mf/nCs+Jk/f3647rrrSk7Yna5comIoGUpFpRF3JExjWB/79d133w2/+MUvOrxebHcqs0oNt0t+R9VPYWVPMck+Yr0ZZleI6i2Gw9UD8cEwy0Ljxo2L2whZTKbP/qTyjsqgb33rW+Gwww5r9Xf+xvDIQlTo8Vwq9uo5LDSZ040J4vkupVGlxvA7PvOcc84p+nriufBul5/85CdjBSTtBdVbkiQ1O4faSZKUP+Fm6A9zzDD0jCFgDz/8cBzmxBwuX/3qV+OwG/6WTujUgrmUCpEsueWWW+K/mVOmGBIOJMceeeSROFSNoUXM+QOGeJGM4AS53BAs5sxhXp999tknDmci0dSnT/bdgPauX5JU+utf/xqH2+21114tz2efsB+YT4ehZwleP2fOnDjsqdRE8MzNA4Yi1sPBBx8ch/UxhJJkSpI0Ya4jkg4M36p2WN8mm2wShxmSsCJBmcxRlRUm1GffsN3mzp0bE3JgPZLhcPXE+++8884xDrnD3BlnnFH0eW+++Wbcvwwj/de//tUyVxnDDeu5XAzXvOeee1omiy+G5BND/EolkJh7qhiG0hITDGWVJKnZmXiSJKkgQcMDVD9xUs7cOlTQMLkxlQ9UatSKyqlBgwYVPcleuHBh/PeYMWMqvg/PZzJoEijf/va347JRUUOCgyohJotmriZOiNNVNsyX9Pjjj8dEA4+BAwfGRAfvQzKKE+UstHf98PnPfz587Wtfi9v95ZdfjhN6gzmVSEaQyFpuueVaXvuPf/wj/p85gQonrS5E8qoemCOJeYFIZF1++eXh2GOPbanIIn6ofilWWVQMCbMf//jHMb4OP/zw+KAijGofEot77LFHu5OehZjrioQY27uUZL/VA9uC5A4TgFNtRTKx2D5i/i0mi3/rrbcyXy7WnUQn1lhjjbJ36yuVQCp1d0Lm6ELy/pIkNTMTT5IklcCJMcmZq666Kk7mfeONN4brr7++XYknEj2lKoISpSZiTqPKJ3H66aeHr3zlKzGJwITGVG+Q8OBBRRRVGkzODCY+5w5dnPiTyOG5VHXxf4YAkpgikVVvHVk/lp3kExVAV1xxRazUQnLHwMJKouSzWNfkjnOlVDvxdDWoiiHxxHKReCLJQhIq+VstjjjiiLjOxBr7lAdJTx7chY4KsFqqoNLbP0EShaQdk59zBzkSj9wBjqozkpW33XZb3H71vCsb24XvEXd84zuU3s/puy4ybI2qL6qhSJ6S2CFhy3eRSb75eyPdLa6aIZSSJDU7E0+SJFVh++23j8kAhiTVEwkQklIkAZhvptaECAkDkhU88MADD4R99903/p9heN/97ndbnsvJOxVOyXAzqjFIljB3Dkmd3XffvaXCo1HWj+QSiSeWk2Vk+COVW1Q/Fc7Jk1RUMfQwSU51BhJF3GmNIWAk8ljXWbNmxTvFFc5pVQ3uzvblL385PsBdCrmr2vTp02MCJ0lqJdVPixYtKvo+VIUlw+bSSFSyjLvsskvR4W7JkLZ6Of/882Msjhw5MlbbJXdaLHbnQ5JKxDIJsayXizghAcb8Y1TLFc4Jlq6iS6rwJElS7bxMI0lqetVUUHCreSTDvRLJyT/zxbQH1R1JAoXbt3cUlU5MlAzmpiqHuZSomOKEm8oYEjr11tH1mzBhQpzviKRDUtFVasJu1p3EFnMIPfXUU6GzUCnEnEUgSZZMKs78U8l8VR0dzpdUo6X36fDhw2P8zZs3L7z++uttXnfrrbcWjUueD4bxFfsuXHnllaFeSNYyXJLJ0pm3qdTQtErLRZKUOdaKae93kDnOiC+USlQm+3Ly5Mk1vbckSfqAiSdJUtOjIoNERrEJpzkRv/baa8N5550Xf04SDIkkEdWRRAdDqDh5Zggf1SzFhkcx0TLLkeBOb3/5y1/aPJcqFyY1LjyBp9ooSZ6lUU2TVJIUO+Gvh/asX1oypI47CyZJkWKTQXOXOT6LfUY1D8PUCnGnsTvuuCNOJF9PyZA6kmvJXfhqHWbHct18880tk2knWB8muC7cR6wvd+rDCSec0Gq7PvbYY3GOqGKS+by4a126Ioptc+KJJ9Zt4nUm6mdSeBKEfNbGG29c9vnJchEj6Soukk4kU1988cWir0u+g8Rx4bar5Kijjor///nPfx7vbJlGMorEGduZijZJktQ+DrWTJDU9TlaZQ4gHVSTjx4+PlTPc+p3qGe5QBoawFSYTuFMdVTgMDWLuJCYRZ0gbQ6OYdLoazCPFhNkkU3iQRGAuHJaFKpAnnngiTq7NvDzJ0C3mamKic5aT5eVzOVknoUL1C0OD0sOVTj311Jj4oXqGE3yGv3EHOJIzVIrst99+cTlqxd3xSmEuIpIw7Vm/NJaN1/AeINkyduzYop9JsoUEG5Oub7XVVmG99daLz2V9X3311VgxxH4l0VBu2duzHVgn4gUkWWrdnlScMbE2E1PzWoamMSSO4YUM3Rs2bFj43ve+1+o17FcSkEzKTUxQvcYcTsznRcXVtGnT4mvTmDuJu/4xpxLVZEz2zXxazPlFTFBdVeqOc7U47rjj4txoTNxNwrBUJVV6zi5imgn9eQ37j4o55rViO5D84e+FqKLizois8wYbbBD/TaUZ3w3mQStnypQpMbbYjlTmMTk/70dClu3O55PwJI4kSVI75SRJanILFy7MXX/99bkjjjgit8UWW+RGjx6d69u3b27gwIG5tdZaK7fXXnvlbrnllpKvv+iii3KbbLJJbtCgQYzZi4/LLrss/u3FF1+MP6+22moVl4PnHnnkkbn1118/N3jw4NyAAQPi6yZNmpQ7/fTTc88//3zLcx955JHcsccem5swYUJu1KhRuX79+uWGDx+e23TTTXOnnXZabu7cua3ee+rUqbkDDzwwvvfyyy+f69+/f3zvKVOm5K677rrc0qVLq95ed955Z8t6lnsUrnMt61doxx13bLNty7nnnnty++yzT3x/1nXIkCG5tddeO7fzzjvnLr744ty8efNaPZ/n8d4sY3udffbZLct47rnnln3uxIkT4/PYlgnW/+STT85ts802uVVXXTVun+WWWy634YYbxn09e/bsou81ffr03Pbbb58bOnRojNmNNtood/7558d9Wmq9Fi1alDv++ONz48aNi5+z0korxW3z4IMPtuxfljGtXCwX+5xkHSs90t54443coYceGr937LeRI0fm9t1339xzzz0X9zvP33///dt8/qxZs3J77713bsSIEbk+ffq0Wc6TTjop/o7/F8P3mxhbYYUV4utXWWWV3B577JG77777qt5/aZU+T5KkZtKL/7Q3aSVJkiRJkiSV4hxPkiRJkiRJyoSJJ0mSJEmSJGXCxJMkSZIkSZIyYeJJkiRJkiRJmTDxJEmSJEmSpEyYeJIkSZIkSVIm+lTzpKVLl4Y5c+aEIUOGhF69emWzJJIkSZIkSeoWcrlcWLRoURg5cmTo3bt3xxJPJJ3GjBlTz+WTJEmSJElSNzd79uwwevTojiWeqHRK3mzo0KH1WzpJkiRJkiR1OwsXLoxFSknOqEOJp2R4HUknE0+SJEmSJElCpSmZqko8JV599dXw1ltv1fISKVNLliwJffrUFMZS5oxLNSLjUo3IuFQjMi7ViHKzZoW+CxZ09WJIrSyqMj9UU4t62WWXhQEDBtTyEkmSJEmS1E5D588Ph593Xui7ZElXL4rUSv8Q6p94GjduXFhxxRVreYmUmTfffDPMnDkzrLPOOmHQoEFdvThSZFyqERmXakTGpRqRcalGlHvoIZNO6tZqSjzR+FaaNErqLG+//Xb8v3GpRmJcqhEZl2pExqUakXGpRrS4f7V1JVJj6t3VCyBJkiRJkqSeycSTJEmSJEmSMmHiSZIkSZIkSZkw8SRJkiRJkqRMmHiSJEmSJElSJkw8SZIkSZIkKRMmniRJkiRJkpQJE0+SJEmSJEnKhIknSZIkSZIkZcLEkyRJkiRJkjJh4kmSJEmSJEmZMPEkSZIkSZKkTJh4kiRJkiRJUiZMPEmSJEmSJCkTJp4kSZIkSZKUCRNPkiRJkiRJyoSJJ0mSJEmSJGXCxJMkSZIkSZIyYeJJkiRJkiRJmTDxJEmSJEmSpEyYeJIkSZIkSVImTDxJkiRJkiQpEyaeJEmSJEmSlAkTT5IkSZIkScqEiSdJkiRJkiRlwsRTBr72ta+FP//5zy0/T5o0Kfz1r3/N5LNmzpwZdt999/DOO+9k8v7qeu+9917Ye++9w5NPPhl/fuWVV2JMPffcc6HRpWO/muV+5JFH4nMWLVoUf77vvvvCl770pbB06dJOW2Z1bjy31w9+8IPwv//7vy0/f/3rXw8//elPy77mC1/4Qvjtb3/bshz8/Mwzz3RoOdS946CrdYf2vNna4cI+XGfKsr+YlVtuuSV8+tOfrul7Wo1K39P58+eHnXfeObz++us1vW9PV+s5SGG/qxqXXXZZbBM6QzWflUU7mt5uxlrj2DqEcGUH3+OAEMLOFZ4zLYTQi30f6mNm/v0ezf/89xDC6BDCW6G5mHgq4bHHHgvHHXdc2G233WrqCNxzzz3hX//6V/jkJz8ZOsPqq68e1l133YbpRKu0X//61+GQQw4JU6ZMiQcwOmEvvfRSxdfdeOONYcSIEWH99dcP3c0111wTPvrRj7b79by2T58+XXYSoOpjm3ay0gl/PeP5iCOOCMcee2y7X9+3b994YnPhhRd2aDmaHScF7Pv044tf/GK3iYNmUk2CoCe1w2+88UY49dRTw2c/+9mw/fbbhwMPPLBiormz+3BdpT3Jhkb7ni677LJxv/7yl78MPc37778fLrnkkrDnnnvGdSRJf8UVV4RcLlf3+KUNpq/2oQ99KDQijtM/+tGPyiYxV1pppbgOa6yxRibL0JNjrSvQ6nwjhLBaCGFgCOHjIYQHqnjdjSGE10IIe3bw888JIaT35KT88qSxTK+EEIaFbKwbQtgyhPBBZDcHE08l/Oc//wlrrbVW+MY3CkOxPBq+T33qU6F3787btCQybrjhhrBkyZJO+0zV7tFHH40Jp/PPPz/88Ic/jB2Lb33rW2Wr1ehkXHfddWHHHXcM3dEKK6wQ+vXr16H34PvE90qNiRO53//+97G9rKSe8UwneciQIR16j+222y488cQT4cUXX+zw8jQzLoDwHU0elRKQjRYHPaF6LGvdrR0moXL44YfHhNkZZ5wRLr/88nDooYdWjJWu6MN1t1hopO8p/d8//elPYeHChaEnueqqq2K/ngpOYvd//ud/4u+uvfbauscvF2Hoq/XqRT1G4+A4wXnNoEGDwrBh5U//l1lmmbgOfN+z0lNjrSscHEL4UwjhVyGEJ0II24cQtg0h/LPC684NIRzYgeTF+yEE6naJpmUrPJczl1XyVUpZOTCE8PMQQjOdvXfPI2sn4ArfwQcfHLbaaquqX0MpJleRPv5x8qSl/eMf/whHHnlkzJ5zJY4kxNtvv93yt8mTJ8f3Ag0cP3/3u99teT1XPehQJTbbbLP4PKq01LjOOuuseODiiszYsWPj1b/XXnstPPvssyVfM2PGjDBnzpzwsY99rGJS6ytf+Uo8kd51113DL37xi5ZE5L333huvcpPoAqXIXOnkOYkzzzwzXhmuVOp88803h89//vOxY/PjH/84viedoV122SUm1X71Kw4jH6hULfi3v/0t7LvvvvG7QJL31VdfbfMcvk9sh3/+s9IhSZ2Ndou4Ofroo6u6WlosnpMS+TvvvDNeFScWqAycPXt2TGrR4SbejjnmmJZ2sZqhG1z1pWqV9+OqMR3GQpwIcbX3jjvuaNf6q3WnP3lwdbg7xUGh9i4LQ9E4SWT4O20xbSbD1NKefvrp2Lfg77zP888/3+bz6Qfw3nwGbev3v//9Vp/DyehPfvKTmOCjD8EFDPzmN7+JVT28bo899ohtdNK3oG9CAuatt95qqUyjXce7774bL4iw3Lz2q1/9anx+d26Hr7zyylgFwXF2nXXWidV1m2++eRg1alTNfTi2FRV6vNcOO+wQ9ttvv/DUU0+Fl19+Oe4Lttlhhx3WZtuQOKBSZdttt41VgLfddlurv/N6hkURC/vvv3948MEH2ywTQ3tOPvnkeAzfaaedYqwTn4Xxz7GXCv2k2pDPIr7ocxBDp5xySvwugNfTBwXvyfrxPkkMU8GaVNoQw9OmMeiksvvvvz9uG7YHMfnmm2+2Wc7CYwfPpc9C1X6xYbKLFy+Occt60PfgIkca/akVV1yx2w1PrIQhyBMmTIhtJLHLPiJ+aT/acw6yYMGCcMIJJ8T43WeffWJlVLnqtz/84Q+xDeH5vI62pVi1JHFGRRJ/4zwlaW9Koa2h78l7E/d8P2666aZWy0GbSewmF4bSQ+3496233hqXP2nHeF2xoXZcUOI7ywUO4oe2PPmO0o4fddRRsf1k2Ym9cn3xnhxrnY1L7VzCODM/bG5sCOHk/P9JwpTyRgiBntpOBb/nyHhICGHlEMIAKviI3/zfqGpaNl8pRYVR/xDCSwVD7fj3XfkqqF75x8wSQ+3uyVdHDQohLBdC2IE+Rv5vfwwhTMh/3gohhM+EEF6osC22CyHMy39+szDxVEc0kP379w+rrUbxYHFUt3BA5oTnggsuiB2Khx56KJxzzjktDdvQoUNjIgGPP/54/DmdVOLfG2+8caurFSQyeK66j3//+9/x/+WuAhJTo0ePjld8yg0n4OD6kY98JFx88cWxQ0mCKEkCbbjhhjHukgMy8cPVoyTGisVUMZwo0iEgSXXiiSfGz+Bz+XzilxMzSsP//ndGLldGh/o73/lO7CSx3Bz8iw17WnnllcNyyy1nfDcg9vuWW24Zk9/VKBfPdCg5abroootiIoMTJdpIOoucjNBhvPTSS6tettNPPz3GGCffdIg5CUyfvCf43hhbHcO+4aR3r732iieTJNS7WxwUU+uyUG3ACRqJG9rCLbbYIp5sk2AAJ2UkwagQo6074IADws9/3rqrzcnfN7/5zfDhD384nqDR3pIwSF98AidfHPvPO++8+HxQsUAig+Xmcx5++OGWCwwkWLlgNXjw4JbKNE4Yk+8x7TbtOss9ceLEmPhKlrs7tsNccBk3blw46aST4kURkn2cTLe3D8cFP07COVatuuqqMRYY/sNJPNuY6oykHwdOTokRkiXsDxI8xGKS0CPBw/GPCg1igH2YvhgELh7RX+R7wnuxrwcOHBj3Tbqyif1MUpSLmEkCiddyss7+5DvJRR0+HyTkvve978V/008gFohpkHQitlgehhWRjCTxme4vFEOC6P/+7//C8ccfH84999z4nSuM7bSf/exncXvz3iw3cVVsfh6+T+xHvoOf+9zn4ve4cIqCntiG833l3ID9ChLUbK9yUxeUi18S4lzEJh44ZhMTpSp3eB9imzadeOf4PnXq1KJ9wrvvvjvGHA/6kSR8y+F5t99+e2ynWCaSP8R0Gm0jiSf+XlhJTZvFetC2Ju1YseHa9EtJJlFxz7rwniSgkguwtMV8n/lekXQnIf3tb3+7YuKsJ8ZaZ+OS+Pv5JFEaUXB3mdfxN3oM66R+R/XSlHxCiAjl7INWbpnUc9ijZ4QQLg4hPEX7V/C+tNpcAvtyfmgdjzFFPp8WcJt8Amt6fnlIgv03ov47VxNHYi4f3J5PsOySX8ZyVVUbc7wIzcPEUx1xYF9++eXLlrjS4JLxp1O45pprhk022SQ2jlyFnTdvXuw4brTRRi0Hef5Ppp5OxqxZs2JngitthUkCsvCVOvtqHHQ66URywCQOysUU+7YcTqSGDx8e44gOB1V6XPWmw8bnUIlCYjIdU1xtoiPDQZYDNCdQlRJPdKw5MHPSRLKI59Mp4mSGjjhxOmbMmDZXysstNwd7hj/weq5ucfWzGOO78dCWcYXwy1/mcF2dcvFMh5LOJDFMh5f35ur5BhtsEE/C6TRWG1vEJUlSTtrWW2+9eOLCvzk5KmRsdQxzDJKAJkFC0psrz5xUlOvAN2Ic1GNZOPEm+bbNNtvENo1kPG3v7373u5bvDG0yy8BFJtrRJPmTYAgi7833is/l3yQa+JzkJBQk7qhy5XN4gHZ9/PjxsUKCvgWJB6q2QJKKpBOSyjQSGsQ+cz9xEYyLFLTJVLuwjvy+u35XOCnmGMN2otqYpAUJkT/+kevStffhOL5xwssxjn3Mc6lkSsdHOjlDLHA8I+nFa0hAbb311vH3IKlAAoVEDTFCv4/kWBqVmEm80E/gczgGk9RJf9aAAQNaYiqZ44bYJEkxcuTIGPsklvgu8L0kiZpc8KI6kVign0DflMQTn8F68VrWm2NzYaVRIfqmJKs4MV977bVjlRXrWAzLQHKLBO2mm24a143PLDZ5PevANmQ/Uh3DRbPC7393istqsa7M00R7Q3tCe5BUUrYnfolF3oftSJxxMbJU9RTD+dj/tAPELtufn4v1CWn72X+0HVTIldrnoP2iPaI9o59KfLH/C+ejov9Ksou2iAvvabRZJJOS4YE8+Heh66+/PrZ3JNOJSdaDWE7aStpHlpfvFA+qtjkuVEqw9sRY62xD8omeU2in84mbqflkzge1nG3Nylc1paObWQfvJ2bz1UNr5iuNSEYlSNGfn5+zaVw+eZU2LJ8AGpQfWrdKQeIqQYXWZvn32iiEsF4IgbFHSU9mtxDCrvnKLc6mLs0PI6x0KX5kft2aRXaDYZsQB+1K89mQPCKDn87wk3zggEujzEGDDkhykOcKAgcJ/kaDyNVQDvCFGX6uclTbmVbXY5gEZcCV5kKpNqboWKbH5xMfdCxIKnGlOklmcpLD1Ro6MXQAuLJFTHEwpUOCdPKHTg5XpLDKKqu0qlAgVunApjs5/K7aagKWmyEQaaxHMcZ3Y+HEh8QpV6rZN9UqF8/pK5vEEdJJ2Vpji9jkBChB57LYcEBjq2PSV+DZh3ynOWGhfSk1kXUjxcHZZ5/davhdOjFRy7IwhG3u3Lltjs38/MILL7Q6/qe/M4VtHs/lxLpYEp5kCidQSK9TgqFaVByQ0GB5uLrPtmbOSpITxTCsj/4HQ57TuNhVOK9Kd/qucFJMojFJjJPA45jLkLlSFzg6Gpe8nu3OCS/7+jOf4RSo7STO4O9UHqUTsMVigYtCnDAXLiexkGA5Ck++GRZJxRLvwTE+mZSatpuLR8XwWcRKcsxP0Odk+4EqvWRIPMkGEs4gvtLDGEkIlPqesuy8Z/r4z3cyie1S250+TrHvf3eKy2rRfjKZP8PcSCZyoZBjLtu1o/HL+QcxWmr/cL5ROM0I+2r6dFIDHyjsE6b3OW0qbWuCOGHoJf3FShc5+d52FNuL+Cw15xMX+qn+ol/MMtNWEkOV7lrXE2OtKzAe46AQwqh8kmeTEMJeJOQrDNErPIqRJuTMpe3R8AN8IzaswzLzWXuU+Tv1midyF9gQwtxUpRP1meVuoTIwX5XVLEw81RGdtHrcIYRGmQMMZe4zZ86MVx7pSNJAMjyLRrmwE0nJLFcP1D2SThzAufpKx7NSTHFiUI+YYmgcB2MOxJx88bskpkhMJSitTiRXyMEJXKFiv8viltvEd6XJJdV5OKlh+E+62on9TlKTig06ncVio1w8VxNfxlbjo5KCJHa5uYAaKQ4OOuigNlVHXbUs4IIBlVAMNSnEiV2isA9ApRmV1FT2UOlEpQAXFjjhI4lUKvHE53EyyFCUwkqJwiEw3em7wrYqHHLEz3/5y1/a1YfriuMf+4b+XrE5zNLzqBXu22RKB6pUSFywXpxQ87tyk48nNzphSF5hRWKS0OBvyfyR6QRqsW1R6Q5s1ahmG3enuKwWQ3qpeqJKKUkukvCjIq1U4qnW+O1orJZ7z0984hOtEotU5Zerhkor1VbVotIFMYb8ETdUAnJxlsQt87RVmpy/J8ZaV1grP68Rw9MY8DmCCuN8xVIpK6bmU0q0PkIVN7BOE4RX+iyG3XHEuShfxbQ0n3B6t8Lr5uW3R7Mw8VRHlEuTRafhLzVvDx0frqhygE86dUwiSIcvudrDAYbXM/ae9+SKAkkCJnEmSVDsagFX8piXQY0rmQOCMfEknxgOUQn7n+ECvLbUHUeIqbvuuqvVc4gp4oaDfXqeJ4Z8JEkm4oir48QUwwASSeVT1lju9ASXKDY/FFeXuEKaXHFV16M8vnCeHSaApYydYSjFOqTVxnM9sBxcwWRoFGX2IHmfzKtW2HYaW/XDMBq+rwxjKKWR4oB5i3h0FEl6TtZpe9PHaH5OPps2j8l4adOSE6PCNo9KJtpzqglquUMT68j2ZOhykkBKhtklOLkqPNkk9vkdV/05TpTS3dphqovSQxPBz5xkdqQPVy32Nfs+nSTg5yQZxv9JBlEFkiQUi8UC+5D4TF8EqoQY5wSZ5GVycYuLBWlJhVQ6HqiE4vcsV6mqFOKyo7hISmwzwXOyP/hOsn/KxWAptOGVqmi6G75vhYlgjqvlknn1il/ORdg3aYU/V0L/s3AOPyq3WH4ueFY7L2QxxdqxQpxHMZyTJGmxdpTvIkPDme8KxDwTsDdjrHWlwfkHCaVb88PZShnPcNL8c5MjNq0FMxE+W6HqqZJ+qbmaStkwP3dT69kW/4vbKMzIJ52SWsFy81WlPRlC2D00D+d4KtN5ZqLDZLJDrjTw73Jje+mQkQnnKmMpzAnAlSOy7VzxpaSeyheGNCXl23TEOfhSNZA0cJTJkonnikG6OiW50kmJPyeDalwkm9inXIEk6UiHk0e5sl3m6yBhVO5271zhTib4pnyfxBaTmTLfR9JxoRPCgTgdU8QRMU1nrzCmOgN3E6EqgglI6ShTVl5s/g0643Q0Sg3DU+ejQ0k8pR9cpaTKotycZdXEc70SDlztp9Sf+OGki3leil0FpUqLuwWpfZiYlRMJjkN05pkwmXYnuVLfXeKgHqic4gIRc/PQpjFZNFWmzM0CtgnHd4aoUs3MXT2TOX8SzKfCiSOTV3OyRxvJ3cKoNEkmxi2GYU6cZDE/CwkiElwMKytMGrDd6UeQaGJYFSeZ9EtOO+20WA3EfmTuFyor0kNruls7zPGPZWZSZKrHOb4wuTjbtyN9uFpigeMZCVY+nzkX2b5JdR39NbY9fUFihHYoXW0M9gvLQ8UTf2ffJH3GckOCkgqOJBa4wFN4x1meQyyyj4kF+ry06ywfFfcsO7FHQpP3KTc3Vq34HCZ2pqqH9aEdoDKPdqPWRDQxzDL2tDacu9mxz9g/7HcmqyeGyt1pu17xy10GmQ+MzyN2aUf4uaMXCbjYyn5nX7M+STwXJsgroR3j/Ik2lthNKvDSmGOMYa9Mok87ynrQJiYT03OBlZ/pM9NOMNl6peNCT421rkCSiRaFHgAD3SczcTvze1VIPFH1lL5cPTF/Z7zd8u/D+zEzYa2t1er5IXIzC4bJpR0XQngghHAo/UaSsfm78M3NJ8K4fMDtkZ7P333vv7f8KG8mQ5xp60PzMPFUAh1UhpEkQ0m4Awf/Lnc3Ha5GMBafDk4pnJzR8aVjycSg3HElmWA8jUQAGf0kScABmWQUDX/hHBJ0crl6UI8rUcoOHVAOhN/4xjfiRKTJo9yt3OlE0NEoF1NUNXFSwsGV+cC4gwcTiya3VS4VUyQJuOpKwjOZcLEz0fHlTk0kyhgaQuem2ETVbB864PUov1bXqiae64XJaqkkoG1lglHuKpUengJu1MB30mrR9iPpTZKESXCZoJp2hWRU4bZu5DioF9pzEh6sP0P4SBhx166kipQTbhI8nDTR1pFoYALyNKqmOPGnrWbCW96Hn5kDp9yNS6h2YKgIiS8m5+UiQ+FwPfoOJPxpd0nA8FwwQTAnhCw3xw0ujqSrUbpjO0yVGXHJhO5sD+5Kx40wyk3OXE0frlrEN8N4SCwyLxLzdrKdSbqCfcnyceGJSbbpFxZOLs625oIS+4GELt8xnsdcPuUqoIhvPmvatGlh//33j5XNfEZhv4HtwhBLTtKTO/JxLOZzSDzyWiaCJvlRTYV2LYhVkpgMD2VOKaaUoD9SaU7LQiTVqOpqT6VUI6O94rjEBUv2AxfoaLtoD7KOX/YFE8WTeCIeaMdo12rdN8VQZZSsF3FGEp6ETi2YO42kLW0n7VixRBvHGO6ASKKdPjdtIYnnpPqJuOY8jHaYNplkW6XjQk+Nta5Abdlh+WTTfiGECflkVNtp4j+wTD4x9euC3zNr3ub5OaK449wxVVQvFTo6//68nnEire+b+V9UVN3G3MshhC3yE6TfkB86xpH56vwcVZyhHxlCOKuKz70qhEBteNv7UPZcvXJVDMJOxrRyAC1XpqwQK1iSg3lnJIKogmJSUDqKHCyaCdVnXJnlymFHy+IbGZODcgJCR7DY7cd7Mq5m0TmhcqDeHd+sNEtc9oR45gScatLCiZV7okaLy0aKA3VdO9xocdnZfTj9FwkCKgMZKlrqpgTFkFAj4UtStCfHZVfHLwlPqoUq3RCnJ8sq1mqxePr0sP1x1N40J4baUXP7cA9I1rxLlWII4UrmRAvdH3N1MfsZQ1YL70aZZsVTnXFllQkcK90ZoZ4Hx3322afpkk7NhBNjrtYkd5JpJqwzV6u6S9JJ3SeeSdoz5wRXctW8caDKmqkd7uw+XLNimD/VaMlwPoY6JZNS15IQ3XrrrcsO62029Yrfq6++Og4BZYgaQy2ZL4mqyGZlrDUGUqmXlKhI6m5eCiEc30OSTrVwcvEMlBuDXW+U8HfWZNDqOoW3U24WDJdIJuZVz9EI8cwcKFRxqLnjQJU1WzvcmX24ZsYoCqpoaIuZSJ1qmlqGwfJcbmah+scvQ21JPjH3F5PBM2yUIW7NylhrHKVn6etexuYfzcbEkyRJkqROwUTYDAdTY2K+PkmqN4faSZIkSZIkKRMmniRJkiRJkpQJE0+SJEmSJEnKhIknSZIkSZIkZcLEkyRJkiRJkjJh4kmSJEmSJEmZMPEkSZIkSZKkTJh4kiRJkiRJUiZMPEmSJEmSJCkTJp4kSZIkSZKUCRNPkiRJkiRJyoSJJ0mSJEmSJGXCxJMkSZIkSZIyYeJJkiRJkiRJmTDxJEmSJEmSpEyYeJIkSZIkSVImTDxJkiRJkiQpEyaeJEmSJEmSlAkTT5IkSZIkScqEiSdJkiRJkiRlwsSTJEmSJEmSMmHiSZIkSZIkSZkw8SRJkiRJkqRMmHiSJEmSJElSJkw8SZIkSZIkKRMmniRJkiRJkpSJPrU8+e233w6LFi3KZkmkGr3zzjstcSk1CuNSjci4VCMyLtWIjEs1otzixV29CFLnJZ5mzJgRZs2a1bFPlOrs6aef7upFkNowLtWIjEs1IuNSjci4VCMZOn9+eK9Pn9B3yZKuXhSpXXrlcrlcpSctXLgwDBs2LCaehgwZ0r5PkjKwZMmS0KdPTflTKXPGpRqRcalGZFyqERmXakS5WbNC3wULunoxpFYWvfVWWGu33cKCBQvC0KFDQylVtahJbmrQoEFh8ODB1bxEkiRJkiTVw7rrdvUSSG0sXrgw/r9SPVNViadkXqcxY8ZU83RJkiRJkiQ1AXJGjJLr0FC7pUuXhjlz5sRhdr169ar3MkqSJEmSJKkbIZ1E0mnkyJGhd+/eHUs8SZIkSZIkSbUqnZKSJEmSJEmSOsDEkyRJkiRJkjJh4kmSJEmSJEmZMPEkSZIkSZKkTJh4kiRJkiRJUiZMPEmSJEmSJCkTJp4kSZIkSZIUsvD/90BpHFk1wgwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1500x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mapping: Stress level label (1-based) to 0-based index\n",
    "label_mapping = {\n",
    "    0: \"1 (low)\",\n",
    "    1: \"2 (low-mild)\",\n",
    "    2: \"4 (mild)\",\n",
    "    3: \"5 (mild-moderate)\",\n",
    "    4: \"6 (moderate-high)\",\n",
    "    5: \"8 (high-critical)\",\n",
    "    6: \"9 (critical)\"\n",
    "}\n",
    "\n",
    "# Function to visualize stress level slider\n",
    "def visualize_stress_level_slider(predicted_level_index):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Validate stress level index\n",
    "    if predicted_level_index < 0 or predicted_level_index >= len(label_mapping):\n",
    "        raise ValueError(f\"Invalid stress level index: {predicted_level_index}\")\n",
    "\n",
    "    # Map index to its label\n",
    "    stress_level_label = label_mapping[predicted_level_index]\n",
    "\n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(15, 1))\n",
    "    slider_width = 1.0 / len(label_mapping)\n",
    "\n",
    "    for i in range(len(label_mapping)):\n",
    "        ax.barh(0, slider_width, left=i / len(label_mapping), color='silver', edgecolor='gray')\n",
    "    ax.barh(0, slider_width, left=predicted_level_index / len(label_mapping), color='red', edgecolor='red')\n",
    "\n",
    "    for i, label in label_mapping.items():\n",
    "        ax.text(i / len(label_mapping) + slider_width / 2, 0, label, ha='center', va='center', fontsize=10)\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_title(\"Stress Level Visualization\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "stress_level = stress_level  # Predicted stress level (1-based)\n",
    "# Remap to 0-based index\n",
    "stress_level_mapping = {1: 0, 2: 1, 4: 2, 5: 3, 6: 4, 8: 5, 9: 6}\n",
    "if stress_level not in stress_level_mapping:\n",
    "    raise ValueError(f\"Invalid stress level: {stress_level}\")\n",
    "\n",
    "stress_level_index = stress_level_mapping[stress_level]\n",
    "\n",
    "# Visualize\n",
    "visualize_stress_level_slider(stress_level_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Trying Improvement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input Shape: (None, 13, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check model input shape\n",
    "print(\"Model Input Shape:\", model.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"conv1d_14\" is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (1, 128, 128)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 128, 128, 1), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m features \u001b[38;5;241m=\u001b[39m extract_features(audio_path)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Predict emotions\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m emotion_probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     61\u001b[0m emotion_scores \u001b[38;5;241m=\u001b[39m {clean_emotion_label(label): \u001b[38;5;28mround\u001b[39m(prob, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m label, prob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(emotion_labels, emotion_probs)}\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Transcribe and predict stress\u001b[39;00m\n",
      "File \u001b[1;32md:\\git_projects\\stress_level_detection\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\git_projects\\stress_level_detection\\venv\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"conv1d_14\" is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (1, 128, 128)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 128, 128, 1), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Suppress TensorFlow logs\n",
    "\n",
    "# Path to the saved model\n",
    "model_path = \"models/final_cnnlstm_model.h5\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Emotion names\n",
    "emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Pleasant_Surprise\", \"Sad\"]\n",
    "\n",
    "def clean_emotion_label(label):\n",
    "    return label if label in emotion_labels else label\n",
    "\n",
    "def extract_features(audio_path):\n",
    "    audio, sr = librosa.load(audio_path, sr=None)\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=8000)\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    log_mel_spec = log_mel_spec[:128, :128]  # Ensure correct shape\n",
    "    return np.expand_dims(log_mel_spec, axis=-1)\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "    try:\n",
    "        return recognizer.recognize_google(audio_data)\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Could not transcribe audio\"\n",
    "    except sr.RequestError:\n",
    "        return \"ASR service unavailable\"\n",
    "\n",
    "def predict_stress_level(emotion_scores, transcribed_text):\n",
    "    high_stress_emotions = {\"Angry\", \"Fear\", \"Sad\"}\n",
    "    low_stress_emotions = {\"Happy\", \"Pleasant_Surprise\"}\n",
    "    \n",
    "    emotion_stress = sum(score for emotion, score in emotion_scores.items() if emotion in high_stress_emotions)\n",
    "    emotion_stress -= sum(score for emotion, score in emotion_scores.items() if emotion in low_stress_emotions)\n",
    "    \n",
    "    stress_keywords = {\"deadline\", \"pressure\", \"urgent\", \"stress\", \"work\"}\n",
    "    text_stress = sum(1 for word in transcribed_text.lower().split() if word in stress_keywords)\n",
    "    \n",
    "    total_stress = emotion_stress + text_stress\n",
    "    return \"High\" if total_stress > 1 else \"Moderate\" if total_stress > 0 else \"Low\"\n",
    "\n",
    "# Path to audio file\n",
    "audio_path = \"dataset/test_data/test1.wav\"\n",
    "\n",
    "# Extract features\n",
    "features = extract_features(audio_path).reshape(1, 128, 128, 1)\n",
    "\n",
    "# Predict emotions\n",
    "emotion_probs = model.predict(features, verbose=0)[0]\n",
    "emotion_scores = {clean_emotion_label(label): round(prob, 2) for label, prob in zip(emotion_labels, emotion_probs)}\n",
    "\n",
    "# Transcribe and predict stress\n",
    "transcribed_text = transcribe_audio(audio_path)\n",
    "stress_level = predict_stress_level(emotion_scores, transcribed_text)\n",
    "\n",
    "# Print results\n",
    "formatted_emotion_scores = ', '.join([f\"({emotion} - {score})\" for emotion, score in emotion_scores.items()])\n",
    "print(\"\\nEmotion Scores        :\", formatted_emotion_scores)\n",
    "print(\"Transcribed Text      :\", transcribed_text)\n",
    "print(\"Predicted Stress Level:\", stress_level)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
