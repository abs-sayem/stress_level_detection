{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioStressCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7):  # Number of output classes\n",
    "        super(AudioStressCNN, self).__init__()\n",
    "        \n",
    "        # 1D CNN Layers for feature extraction\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(64)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        # Dropout layer for CNN feature extraction\n",
    "        self.dropout_cnn = nn.Dropout(0.2)\n",
    "\n",
    "        # LSTM Layer for capturing temporal dependencies\n",
    "        self.lstm = nn.LSTM(input_size=256, hidden_size=128, num_layers=2, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Fully connected layers for classification\n",
    "        self.fc1 = nn.Linear(128 * 2, 256)  # Bidirectional LSTM doubles the hidden size\n",
    "        self.dropout_fc = nn.Dropout(0.5)  # Dropout after FC1\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN feature extraction\n",
    "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout_cnn(x)\n",
    "\n",
    "        x = self.relu(self.batch_norm2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout_cnn(x)\n",
    "\n",
    "        x = self.relu(self.batch_norm3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout_cnn(x)\n",
    "\n",
    "        # Transpose for LSTM\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # LSTM for temporal modeling\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        # Take the last time step's output\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AbsSayem\\AppData\\Local\\Temp\\ipykernel_3868\\4085902635.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AudioStressCNN(\n",
       "  (conv1): Conv1d(1, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (conv2): Conv1d(64, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (relu): ReLU()\n",
       "  (batch_norm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch_norm3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout_cnn): Dropout(p=0.2, inplace=False)\n",
       "  (lstm): LSTM(256, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (dropout_fc): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to the saved model\n",
    "model_path = \"models/trained_model_modify_cnn_lstm_audio.pth\"\n",
    "\n",
    "model=AudioStressCNN()\n",
    "# Load the model's state_dict (weights)\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transcribe Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transcribe the audio to text\n",
    "def transcribe_audio(audio_path):\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio = sr.AudioFile(audio_path)\n",
    "    \n",
    "    with audio as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        \n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "    except sr.UnknownValueError:\n",
    "        text = \"Unable to transcribe\"\n",
    "    except sr.RequestError as e:\n",
    "        text = f\"Error: {e}\"\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Level Detection Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict stress level from an audio file\n",
    "def predict_stress_from_file(audio_path):\n",
    "    try:\n",
    "        # Load audio file\n",
    "        audio_data, sr = librosa.load(audio_path, sr=16000)  # Resample to 16 kHz\n",
    "        print(f\"Audio file loaded: {audio_path}, Duration: {len(audio_data)/sr:.2f} seconds\")\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfcc = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=40)\n",
    "        mfcc = np.mean(mfcc.T, axis=0)\n",
    "        mfcc = np.expand_dims(mfcc, axis=(0, 1))  # Add batch and channel dimensions\n",
    "\n",
    "        # Convert to tensor\n",
    "        mfcc_tensor = torch.tensor(mfcc, dtype=torch.float32)\n",
    "\n",
    "        # Predict using the model\n",
    "        output = model(mfcc_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "        # Map prediction to stress level\n",
    "        label_mapping = {0: '1 (Low)', 1: '2 (Low-Mild)', 2: '4 (Mild)', 3: '5 (Moderate)', 4: '6 (Moderate-High)', 5: '8 (high)', 6: '9 (critical)'}\n",
    "\n",
    "        stress_level = label_mapping[predicted.item()]\n",
    "        return stress_level\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {e}\")\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Text: Unable to transcribe\n",
      "Audio file loaded: dataset/test_data/EB0_anger_10.wav, Duration: 2.04 seconds\n",
      "Predicted Stress Level: 6 (Moderate-High)\n"
     ]
    }
   ],
   "source": [
    "# Input from the user\n",
    "audio_file_path = \"dataset/test_data/EB0_anger_10.wav\"\n",
    "text=transcribe_audio(audio_file_path)\n",
    "print(f\"Audio Text: {text}\")\n",
    "# Predict stress level\n",
    "predicted_stress_level = predict_stress_from_file(audio_file_path)\n",
    "print(f\"Predicted Stress Level: {predicted_stress_level}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
